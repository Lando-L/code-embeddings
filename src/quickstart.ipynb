{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "The aim of this notebook is to give you an understanding of how to use this project to predict function names from their function bodies.\n",
    "\n",
    "We start with the setup of the global variables, we proceed with reading the model and other necessary files from disk and finally we feed our example functions into our model to evaluate the results.\n",
    "\n",
    "## Setup\n",
    "\n",
    "During the setup we import the necessary libraries and set the values for our model. If you have trained a separate model, verify your model parameters coincide with default settings of this notebook.\n",
    "\n",
    "> If you run this notebook yourself, make sure to correctly set the `VARIABLES` in the next cell according to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PY150_PATH = '/tmp/py150'\n",
    "PY150_DICT_NAME = 'extracted.dict.c2s'\n",
    "\n",
    "EXAMPLE_PATH = '/code-embeddings/examples'\n",
    "EXAMPLE_C2S_NAME = 'examples.c2s'\n",
    "\n",
    "CHECKPOINT_PATH = '/code-embeddings/checkpoints/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1752: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "from preprocessing import dataset\n",
    "from preprocessing import vocabulary\n",
    "from training import loss, mask, schedule\n",
    "from training.model import transformer\n",
    "from evaluate import evaluate\n",
    "\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_parser = ArgumentParser()\n",
    "arg_parser.add_argument('--dict', required=True)\n",
    "arg_parser.add_argument('--data', required=True)\n",
    "\n",
    "arg_parser.add_argument('--num-paths', type=int, default=100)\n",
    "arg_parser.add_argument('--num-tokens', type=int, default=10)\n",
    "arg_parser.add_argument('--num-targets', type=int, default=10)\n",
    "\n",
    "arg_parser.add_argument('--num-layers', type=int, default=2)\n",
    "arg_parser.add_argument('--num-heads', type=int, default=4)\n",
    "arg_parser.add_argument('--embedding-size', type=int, default=32)\n",
    "arg_parser.add_argument('--dense-size', type=int, default=64)\n",
    "arg_parser.add_argument('--dropout-rate', type=float, default=.2)\n",
    "\n",
    "args = arg_parser.parse_args(\n",
    "    [\n",
    "        '--dict',\n",
    "        f'{PY150_PATH}/{PY150_DICT_NAME}',\n",
    "        '--data',\n",
    "        f'{EXAMPLE_PATH}/{EXAMPLE_C2S_NAME}'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files\n",
    "\n",
    "After the setup we continue the script with loading the model, the example file and the embeddings lookups.\n",
    "\n",
    "### Embedding lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtoken2count, path2count, target2count, max_contexts = vocabulary.load(args.dict)\n",
    "\n",
    "idx2sub, sub2idx = vocabulary.to_encoder_decoder(subtoken2count, special_tokens=[vocabulary.PAD, vocabulary.UNK])\n",
    "idx2path, path2idx = vocabulary.to_encoder_decoder(path2count, special_tokens=[vocabulary.PAD, vocabulary.UNK])\n",
    "idx2tar, tar2idx = vocabulary.to_encoder_decoder(target2count, special_tokens=[vocabulary.PAD, vocabulary.UNK, vocabulary.SOS, vocabulary.EOS])\n",
    "\n",
    "token_table = vocabulary.to_table(sub2idx, sub2idx[vocabulary.UNK])\n",
    "path_table = vocabulary.to_table(path2idx, path2idx[vocabulary.UNK])\n",
    "target_table = vocabulary.to_table(tar2idx, tar2idx[vocabulary.UNK])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = dataset.create(\n",
    "    args.data,\n",
    "    args.num_paths,\n",
    "    args.num_tokens,\n",
    "    args.num_targets,\n",
    "    token_table,\n",
    "    path_table,\n",
    "    target_table\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer.Transformer(\n",
    "    args.num_paths,\n",
    "    args.num_tokens,\n",
    "    args.num_layers,\n",
    "    args.num_heads,\n",
    "    args.embedding_size,\n",
    "    args.dense_size,\n",
    "    len(idx2path),\n",
    "    len(idx2sub),\n",
    "    len(idx2tar),\n",
    "    1000,\n",
    "    args.dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!\n"
     ]
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_PATH, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Finally we use our model to make predictions of names for the functions defined in the example file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_fn = partial(evaluate, num_targets=args.num_targets, tar2idx=tar2idx, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real function name: count_occurences\n",
      "Predicted function name: <UNK>\n",
      "\n",
      "Real function name: contains\n",
      "Predicted function name: is_element_present\n",
      "\n",
      "Real function name: index_of\n",
      "Predicted function name: get_item\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X, y in dst:\n",
    "    y_hat, weights = evaluate_fn(X)\n",
    "    y = tf.gather_nd(y, tf.where(y > 3))\n",
    "\n",
    "    real = '_'.join([idx2tar[i] for i in y.numpy()])\n",
    "    predicted = '_'.join([idx2tar[i] for i in y_hat.numpy()])\n",
    "\n",
    "    print(f'Real function name: {real}')\n",
    "    print(f'Predicted function name: {predicted}')\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
